```{r, echo=FALSE}
knitr::opts_chunk$set(results = "hold", fig.show = "hold", fig.align = "center", message = F, warning = F)
```

Practical Machine Learning Assignment
=====================================

## Introduction

In this project, the goal was to use data from accelerometers on the belt, forearm, arm, and dumbell
of 6 participants to predict the manner in which they did the exercise. 
They were asked to perform barbell lifts correctly and incorrectly in 5 different ways.

## Data

```{r}
train <- read.csv("pml-training.csv", na.strings = c("", "NA"), stringsAsFactors = F)
test <- read.csv("pml-testing.csv", , na.strings = c("", "NA"), stringsAsFactors = F)
train <- within(train, {classe <- factor(classe)})
```

The data was obtained from the [Human Activity Recognition](http://groupware.les.inf.puc-rio.br/har) 
reasearch by Groupware Technologies.
The training dataset containted `r nrow(train)` observations with `r ncol(train)` variables.
The test dataset had `r nrow(test)` observations and the same number of variables.
The variable of interest was `classe`, which classified the manner the participants performed
the dumbbell bicep curl. It took on 5 values: A, B, C, D and E.

## Variable Cleaning

On reviewing the datasets, it was observed that many of the measurements 
had missing values for the majority of the obsersvations. 
A check revealed that measurements had the same number of missing values,
suggesting that the missing values originated from the same large number of observations.
Hence, it would not be appropriate to remove those observations.
Rather such variables would be removed since they would not be useful for prediction. 

```{r}
sumNA <- apply(sapply(train, is.na), 2, sum)
table(sumNA)
```

The first 7 variables contained information such as name of participants, time, etc.
which would not be useful and were also removed.

```{r}
names(train)[1:7]
```


```{r}
train_sub <- subset(train, select = (sumNA == 0))[, -(1:7)]
test_sub <- subset(test, select = (sumNA == 0))[, -(1:7)]
```

The resulting datasets had `r ncol(train_sub)` variables including the variable `classe`. 
The structure of the training dataset was shown below. The test dataset was similar.

```{r}
str(train_sub)
```

## Model Building

I chose to build the model using random forest due to its good performance. 
The tuning parameter for random forest was `mtry`,
which could take the values of integers up to the number of predictors in the dataset, 
in this case, it was `r ncol(train_sub)-1`.
Due to the large dataset, I faced the issue of a very time consuming model building process.
As a result, I broke down the model building to a few steps
to select the best tuning parameter and finalise the model.

The `doParallel` package was used to enable parallel computing, 
which would be extremely useful in speeding up the computation
since the `caret` package allowed the use of parallel computing by default.

```{r}
library(doParallel)
cl <- makeCluster(detectCores())
registerDoParallel(cl)
```

### Initial Model Selection

The first step was to go through the whole range of possible values of `mtry` over a coarse grid.
The range of values of `mtry` that produced models with performance above a threshold would 
then be used at the next stage over a finer grid. The initial grid was as followed.

```{r}
print(grid <- seq(1, 52, by = 2))
```

As this would be an exploratoy stage, I used 2-fold cross validation to estimate the accuracy 
of the models, on which their performance was evaluated.
In addition, I set the number of trees to grow, `ntree`, to a smaller number of 200
to further reduce computation times.

```{r rf1}
library(caret)
set.seed(1)
seeds <- c(replicate(2, sample.int(1000, length(grid)), simplify = F), sample.int(1000, 1))
modelRF1 <- train(classe ~ ., data = train_sub, method = "rf", ntree = 200,
                 trControl = trainControl(method = "cv", number = 2, seeds = seeds),
                 tuneGrid = data.frame(mtry = grid))
modelRF1$times$everything
plot(modelRF1$finalModel, log = "y")
```

The above plot showed that the error rate was stable enough for `ntree` set to 200.

```{r results1}
plot(modelRF1)
gridrng <- with(modelRF1$results, range(mtry[Accuracy > 0.99]))
```

From the above plot, it could be observed the way the accuracy of the models varied 
as the tuning parameter was changed. The models with `mtry` in the range of (`r gridrng`)
had accuracies above 99% and would be used for the next stage of model building.
The model performance deteriorated quickly beyond that range.
The grid of `mtry` for the next stage was shown below.

```{r}
print(grid <- gridrng[1]:gridrng[2])
```

### Final Model Selection

The final model selection was done using the finer grid of `mtry` selected in the previous step.
Since this step was used only to select the best value of the tuning parameter,
`ntree` was also set to 200 to reduce computation times.
However, I increased the number of folds of cross validation to 5 for better estimates of accuracy.

```{r rf2}
set.seed(2)
seeds <- c(replicate(5, sample.int(1000, length(grid)), simplify = F), sample.int(1000, 1))
modelRF2 <- train(classe ~ ., data = train_sub, method = "rf", ntree = 200,
                  trControl = trainControl(method = "cv", number = 5, seeds = seeds),
                  tuneGrid = data.frame(mtry = grid))
modelRF2$times$everything
plot(modelRF2)
```

```{r}
print(best <- modelRF2$bestTune$mtry)
```

The final selected `mtry` value was `r best`.

### Final Model

With the tuning parameter selected, it would be time to build the final model.
A 10-fold cross validation was used so that the out-of-sample error rate could be estimated.
`ntree` was left as the default value of 500.

```{r}
set.seed(3)
seeds <- as.list(sample.int(1000, 11))
modelRF3 <- train(classe ~ ., data = train_sub, method = "rf",
                  trControl = trainControl(method = "cv", seeds = seeds),
                  tuneGrid = data.frame(mtry = best))
modelRF3$finalModel
modelRF3$times$everything
print(results <- modelRF3$results)
```

From the accuracy, the error rate was estimated to be **`r 100 * (1 - results$Accuracy)`%**.
This would be an extremely good prediction performance.
The final model was then used to make prediction on the test dataset.
As an exercise, the models produced in the initial 2 steps were also used 
to make prediction to compare their outcomes.

```{r}
predRF1 <- predict(modelRF1, test_sub)
predRF2 <- predict(modelRF2, test_sub)
predRF3 <- predict(modelRF3, test_sub)
data.frame(predRF1, predRF2, predRF3)
```

It turned out all 3 models produced the same predictions.
The prediction outcomes were then saved into separate files as instructed.

```{r}
pml_write_files = function(x){
    n = length(x)
    for(i in 1:n){
        filename = paste0("problem_id_",i,".txt")
        write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
    }
}
if(!file.exists("predictions")) dir.create("predictions")
setwd("predictions")
pml_write_files(predRF3)
setwd("..")
stopCluster(cl)
```